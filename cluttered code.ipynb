{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/abhiksark/MajorProject/blob/master/cluttered%20code.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "B4zt5FHN23xS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C-l1zVwj2rvh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "frtvbwuj26gY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "!pip install -U -q PyDrive ## you will have install for every colab session\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jw4njDja3IJS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir /content/.kaggle\n",
        "#!mv /content/.kaggle kaggle.json\n",
        "#!ls\n",
        "#!chmod 600 ~ kaggle.json\n",
        "!mv kaggle.json /content/.kaggle\n",
        "!ls /content/.kaggle/\n",
        "!chmod 600 ~ /content/.kaggle/kaggle.json\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UfRtQLQg3Ova",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!kaggle competitions list -s health\n",
        "!kaggle competitions download -c diabetic-retinopathy-detection -p datalab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jj6uamgt5TCV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install p7zip-full"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dDQvtK0V6ux2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!7z x datalab/train.zip.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DQ3jxusI3b1k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls train | head -4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GWUUoSek9ONA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"from google.colab import files\n",
        "for i in range(11,59):\n",
        "  try:\n",
        "    files.download(\"train/100\"+str(int(i))+\"_left.jpeg\")\n",
        "    files.download(\"train/100\"+str(int(i))+\"_right.jpeg\")\n",
        "  except:\n",
        "     pass\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yKyOlCzw3fdw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd \n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h3PBKsUK32lI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unzip datalab/trainLabels.csv.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E73RAbDG4QJ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "026bc9b0-aae1-4142-c96a-42e48743b3ce"
      },
      "cell_type": "code",
      "source": [
        "trainLabels = pd.read_csv(\"trainLabels.csv\")\n",
        "trainLabels.head()\n",
        "trainLabels[\"level\"].hist()\n",
        "trainLabels[\"level\"].unique()\n",
        "listing = os.listdir(\"train\") \n",
        "len(listing)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35126"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFKCAYAAAA0WNeQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAIAx/wdxaXMQE56Y8gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f1e2f243630>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "mBhQmes-4ljy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "listing_test=listing[:15000]\n",
        "listing_val=listing[15000:20000]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vrLNneUYDbZk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu Apr 19 23:09:09 2018\n",
        "\n",
        "@author: abhik\n",
        "\"\"\"\n",
        "\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Standard resolution of images after processing.\n",
        "STD_RES = 756\n",
        "\n",
        "# Thresholding parameter.\n",
        "THRESH = 30  # For edge detection\n",
        "\n",
        "# Canny edge detection parameters.\n",
        "LOW_THRESH = 0\n",
        "MAX_THRESH = 255\n",
        "KERNEL = 3\n",
        "\n",
        "# Hough Circle Transform parameters.\n",
        "DP = 2  # Inverse accumulator ratio.\n",
        "MD = STD_RES  # Minimum distance between circles.\n",
        "P1 = 140\n",
        "P2 = 30\n",
        "MIN_R = int(STD_RES * 0.4)\n",
        "MAX_R = STD_RES\n",
        "\n",
        "# Blob detection parameters (for notch detection).\n",
        "BLOB_PARAMS = cv.SimpleBlobDetector_Params()\n",
        "BLOB_PARAMS.minThreshold = 0.0\n",
        "BLOB_PARAMS.maxThreshold = THRESH\n",
        "BLOB_PARAMS.thresholdStep = THRESH / 2\n",
        "BLOB_PARAMS.filterByArea = False\n",
        "BLOB_PARAMS.filterByColor = False\n",
        "BLOB_PARAMS.filterByConvexity = False\n",
        "BLOB_PARAMS.filterByInertia = True\n",
        "BLOB_PARAMS.minInertiaRatio = 0.05\n",
        "BLOB_PARAMS.maxInertiaRatio = 1\n",
        "\n",
        "\n",
        "def preprocess_image(path):\n",
        "    \"\"\"\n",
        "    Loads an image, converts to grayscale, flips the image if necessary based\n",
        "    on which eye it is and if there is a notch present, and equalizes the\n",
        "    image's histogram.\n",
        "    :param str path: Path to an image.\n",
        "    :rtype: numpy.ndarray\n",
        "    \"\"\"\n",
        "    # Loading the image also converts it to grayscale.\n",
        "    img = load_image(path)\n",
        "    img_thresh = threshold(img)\n",
        "\n",
        "    # Two-part notch-detection. Notch could be in upper-right quadrant, or it\n",
        "    # could be in the bottom-right quadrant. Try the upper-right corner first -\n",
        "    # if it's not there, try the bottom-right. If still no notch is detected,\n",
        "    # assume there is no notch present and do no inversion.\n",
        "    if detect_notch(img, img_thresh):\n",
        "        cv.flip(img, -1, img)\n",
        "        print(\"Notch detected in image {}.\".format(path.split('/')[-1]))\n",
        "    else:\n",
        "        vert_flip = cv.flip(img, 0)\n",
        "        vert_flip_thresh = cv.flip(img_thresh, 0)\n",
        "\n",
        "        if detect_notch(vert_flip, vert_flip_thresh):\n",
        "            cv.flip(img, -1, img)\n",
        "            print(\"Notch detected in image {}.\".format(path.split('/')[-1]))\n",
        "\n",
        "    # Examine the file name and flip the eye horizontally if it's a left eye.\n",
        "    if \"left\" in path:\n",
        "        cv.flip(img, 1, img)\n",
        "\n",
        "    # Finally, equalize the image.\n",
        "    cv.equalizeHist(img, img)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def load_image(path, grayscale=True, equalize=False, resize=True):\n",
        "    \"\"\"\n",
        "    Loads an image, transforms it to grayscale, and resizes it. Optionally\n",
        "    equalizes the image's histogram. Equalization seems to play poorly with\n",
        "    preprocessing however, so by default it is turned off.\n",
        "    :param path: Path to the image file.\n",
        "    :param grayscale: Flag for converting image to grayscale.\n",
        "    :param equalize: Flag for equalizing the image's histogram.\n",
        "    :param resize: Flag for resizing the image to standard resolution.\n",
        "    :rtype: np.ndarray\n",
        "    \"\"\"\n",
        "\n",
        "    img_in = cv.imread(path, cv.IMREAD_GRAYSCALE if grayscale else -1)\n",
        "    if equalize:\n",
        "        cv.equalizeHist(img_in, img_in)\n",
        "\n",
        "    if resize:\n",
        "        img_in = bb_resize(img_in, threshold(img_in))\n",
        "\n",
        "    return img_in\n",
        "\n",
        "\n",
        "def threshold(img):\n",
        "    \"\"\" Thresholds image according to global parameter. \"\"\"\n",
        "    _, output = cv.threshold(img, THRESH, 255, cv.THRESH_BINARY)\n",
        "    return output\n",
        "\n",
        "\n",
        "def hough_circles(img):\n",
        "    \"\"\"\n",
        "    Apply Hough Circle Transform using global parameters and returns data in\n",
        "    a nice list-of-tuples format. If no circles are found, the empty list is\n",
        "    returned.\n",
        "    :param np.ndarray img: The image to search for circles.\n",
        "    :returns: List of tuples of the form (x, y, radius)\n",
        "    :rtype: list[(int, int, float)]\n",
        "    \"\"\"\n",
        "    global DP, MD, P1, P2, MIN_R, MAX_R\n",
        "    img = img.copy()\n",
        "\n",
        "    # Black out the NW, SW, and SE quadrants to force Hough detection to align\n",
        "    # to notch edge. This is done to hopefully reduce the amount of \"edge\" that\n",
        "    # is left over after subtraction.\n",
        "    h, w = img.shape\n",
        "    half_h, half_w = (int(h / 2), int(w / 2))\n",
        "    cv.rectangle(img, (0, 0), (half_w, h),\n",
        "                 (0, 0, 0), thickness=cv.FILLED)\n",
        "    cv.rectangle(img, (0, h), (w, half_h),\n",
        "                 (0, 0, 0), thickness=cv.FILLED)\n",
        "\n",
        "    circles = cv.HoughCircles(img, cv.HOUGH_GRADIENT, DP, MD,\n",
        "                              param1=P1, param2=P2,\n",
        "                              minRadius=MIN_R, maxRadius=MAX_R)\n",
        "\n",
        "    output = []\n",
        "    if circles is not None:\n",
        "        circles = circles[0]  # Why are tuples buried like this? Bug?\n",
        "        for c in circles[0:]:\n",
        "            output.append((c[0], c[1], c[2]))\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def bb_resize(img, img_thresh):\n",
        "    \"\"\"\n",
        "    Resizes an image using bounding boxes. This is done by thresholding the\n",
        "    image and then calculating its bounding box. The shorter dimension of the\n",
        "    bounding box is then expanded (with black pixels) to make the bounding box\n",
        "    square. The pixels in the bounding box are moved to a new image, which is\n",
        "    then resized to a standard resolution.\n",
        "    This effect of this process should be that any eyeball image is roughly\n",
        "    centered at the same position and about the same size. This is important for\n",
        "    notch detection so that a small square can be placed approximately over\n",
        "    where the notch should be in a standardized image.\n",
        "    \"\"\"\n",
        "    x, y, w, h = cv.boundingRect(img_thresh)\n",
        "\n",
        "    # If no bounding rectangle was able to be formed, then the image is\n",
        "    # probably completely unusable. Simply resize to standard resolution and\n",
        "    # move on.\n",
        "    if (w == 0) or (h == 0):\n",
        "        return cv.resize(img, (STD_RES, STD_RES), interpolation=cv.INTER_AREA)\n",
        "\n",
        "    # Destination canvas is square, length of max dimension of the bb.\n",
        "    max_wh = max(w, h)\n",
        "    img_expand = np.zeros((max_wh, max_wh), dtype=np.uint8)\n",
        "\n",
        "    # Copy the bounding box (region of interest) into the center of the\n",
        "    # destination canvas. This will produce black bars on the short side.\n",
        "    diff_w = max_wh - w\n",
        "    diff_h = max_wh - h\n",
        "    half_dw = int(math.floor(diff_w / 2.0))\n",
        "    half_dh = int(math.floor(diff_h / 2.0))\n",
        "    roi = img[y:y + h, x:x + w]\n",
        "    img_expand[half_dh:(half_dh + h), half_dw:(half_dw + w)] = roi\n",
        "\n",
        "    # Resize to our standard resolution.\n",
        "    return cv.resize(img_expand, (STD_RES, STD_RES), interpolation=cv.INTER_AREA)\n",
        "\n",
        "\n",
        "def detect_notch(img, img_thresh):\n",
        "    \"\"\"\n",
        "    Detects if a notch is present in the image, and if so, returns True.\n",
        "    First, the Hough Circle Transform is applied to find a circle corresponding\n",
        "    to the entire eyeball. This circle is subtracted from the thresholded\n",
        "    image of the eyeball. Ideally what is left at this point will be either\n",
        "    a notch, or nothing. Since we will likely pick up \"shreds\" left from the\n",
        "    edges of the subtraction, we contract and dilate at this point to remove\n",
        "    leftovers.\n",
        "    A region of interest is positioned over where notches appear usually,\n",
        "    which is at about the 45 degree mark on the eyeball in the NE quadrant.\n",
        "    Blob detection is run over this ROI. If a blob is detected, it is assumed\n",
        "    that the blob is a notch, and the function can return True.\n",
        "    \"\"\"\n",
        "    circles = hough_circles(img)\n",
        "\n",
        "    # Paint out the first circle detected. Assume that only one circle was\n",
        "    # detected for whole image. If no circles are detected, fail fast and just\n",
        "    # return false.\n",
        "    if circles:\n",
        "        x, y, r = circles[0]\n",
        "        cv.circle(img_thresh, (x, y), r, (0, 0, 0), cv.FILLED)\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "    # Erode what's left to try and remove edges.\n",
        "    img_thresh = cv.erode(img_thresh, np.ones((3, 3), np.uint8))\n",
        "    img_thresh = cv.dilate(img_thresh, np.ones((3, 3), np.uint8))\n",
        "\n",
        "    # Extract a region of interest that is very likely to contain the notch if\n",
        "    # one is present in the image. This corresponds to a small square at about\n",
        "    # the 45 degree mark on the eyeball. Some notches are slightly lower than\n",
        "    # this, so the ROI should be large enough to capture many notch positions.\n",
        "    ratio = 1.0 / 4.0\n",
        "    roi_size = img_thresh.shape[0] * ratio\n",
        "    half_rs = roi_size / 2.0\n",
        "\n",
        "    # Do a little trig to find cartesian coordinates of 45 degrees point.\n",
        "    radius = img_thresh.shape[0] / 2.0\n",
        "    angle = math.pi / 4.0\n",
        "    side = radius * math.sin(angle)\n",
        "\n",
        "    # Get the damned ROI.\n",
        "    x, y = (int(radius + side - half_rs), int(radius - side - half_rs))\n",
        "    roi = img_thresh[y:int(y + roi_size), x:int(x + roi_size)]\n",
        "\n",
        "    # Run blob detection on what's left.\n",
        "    sbd = cv.SimpleBlobDetector_create(BLOB_PARAMS)\n",
        "    keypoints = sbd.detect(roi)\n",
        "\n",
        "    # If keypoints were found, then we assume that a notch was detected.\n",
        "    return bool(keypoints)\n",
        "\n",
        "\n",
        "def draw_hough_circles(img, circles):\n",
        "    \"\"\"\n",
        "    Creates new image from img with circles drawn over it. Will convert the\n",
        "    output image to RGB space.\n",
        "    :param np.ndarray img:\n",
        "    :param list[(int, int, float)] circles: Detected circles.\n",
        "    :rtype: np.ndarray\n",
        "    \"\"\"\n",
        "    if circles:\n",
        "        output = cv.cvtColor(img.copy(), cv.COLOR_GRAY2RGB)\n",
        "        for c in circles:\n",
        "            x, y, r = c\n",
        "            cv.circle(output, (x, y), r, (0, 0, 255), 2)\n",
        "    else:\n",
        "        output = img\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def experiment_threshold(path):\n",
        "    \"\"\"\n",
        "    Launches experiment window for thresholding.\n",
        "    :param str path: Path to the experiment image file.\n",
        "    \"\"\"\n",
        "    img = load_image(path)\n",
        "    _, thresh_img = cv.threshold(img, THRESH, 255, cv.THRESH_BINARY)\n",
        "\n",
        "    # Image windows for this experiment.\n",
        "    t_window = \"Threshold Experiment\"\n",
        "    o_window = \"Original Image\"\n",
        "    cv.namedWindow(t_window, cv.WINDOW_AUTOSIZE)\n",
        "    cv.namedWindow(o_window, cv.WINDOW_AUTOSIZE)\n",
        "\n",
        "    # Callback for parameter slider (instantiated afterward).\n",
        "    def thresh_callback(pos):\n",
        "        #cv.threshold(img, pos, 255, cv.THRESH_BINARY, dst=thresh_img)\n",
        "        #cv.imshow(t_window, thresh_img)\n",
        "        return\n",
        "\n",
        "    # Create the experiment and original image windows.\n",
        "    #cv.createTrackbar(\"Threshold\", t_window, THRESH, 255, thresh_callback)\n",
        "    #cv.imshow(t_window, thresh_img)\n",
        "    #cv.imshow(o_window, img)\n",
        "    #cv.waitKey(0)\n",
        "    return\n",
        "\n",
        "\n",
        "def experiment_hough(path):\n",
        "    \"\"\"\n",
        "    Launches experiment window for the Hough Circle Transformation.\n",
        "    :param str path: Path to the experiment image file.\n",
        "    \"\"\"\n",
        "    # Threshold the image first to get rid of pesky noise.\n",
        "    img = load_image(path)\n",
        "\n",
        "    # Image windows for this experiment.\n",
        "    t_window = \"Hough Circle Transform Experiment\"\n",
        "    cv.namedWindow(t_window, cv.WINDOW_AUTOSIZE)\n",
        "\n",
        "    # Callbacks for parameter sliders (instantiated afterward).\n",
        "    def dp_callback(pos):\n",
        "        global DP\n",
        "        DP = pos\n",
        "        #cv.imshow(t_window, draw_hough_circles(img, hough_circles(img)))\n",
        "        return\n",
        "\n",
        "    def p1_callback(pos):\n",
        "        global P1\n",
        "        P1 = pos\n",
        "        #cv.imshow(t_window, draw_hough_circles(img, hough_circles(img)))\n",
        "        return\n",
        "\n",
        "    def p2_callback(pos):\n",
        "        global P2\n",
        "        P2 = pos\n",
        "        #cv.imshow(t_window, draw_hough_circles(img, hough_circles(img)))\n",
        "        return\n",
        "\n",
        "    # Create the experiment and original image windows.\n",
        "    #cv.createTrackbar(\"DP\", t_window, DP, 10, dp_callback)\n",
        "    #cv.createTrackbar(\"P1\", t_window, P1, 255, p1_callback)\n",
        "    #cv.createTrackbar(\"P2\", t_window, P2, 255, p2_callback)\n",
        "    #cv.imshow(t_window, draw_hough_circles(img, hough_circles(img)))\n",
        "    #cv.waitKey(0)\n",
        "\n",
        "\n",
        "def experiment_edge_detect(path):\n",
        "    img = load_image(path)\n",
        "    img = threshold(img)\n",
        "    edges = cv.Canny(img, LOW_THRESH, MAX_THRESH, apertureSize=KERNEL)\n",
        "\n",
        "    # Image windows for this experiment.\n",
        "    t_window = \"Canny Edge Detect Experiment\"\n",
        "    cv.namedWindow(t_window, cv.WINDOW_AUTOSIZE)\n",
        "\n",
        "    # Callbacks for parameter sliders (instantiated afterward).\n",
        "    def low_thresh_callback(pos):\n",
        "        global LOW_THRESH\n",
        "        LOW_THRESH = pos\n",
        "        edges = cv.Canny(img, LOW_THRESH, MAX_THRESH, apertureSize=KERNEL)\n",
        "        cv.imshow(t_window, edges)\n",
        "        return\n",
        "\n",
        "    #cv.createTrackbar(\"Low Threshold\", t_window, LOW_THRESH, 255, low_thresh_callback)\n",
        "    #cv.imshow(t_window, edges)\n",
        "    #cv.waitKey(0)\n",
        "\n",
        "\n",
        "def experiment_notch_detection(path):\n",
        "    \"\"\" Notch detection via circle subtraction and blob detection. \"\"\"\n",
        "    # Get thresholded image and hough circles.\n",
        "    img = load_image(path)\n",
        "    img_thresh = threshold(img)\n",
        "    circles = hough_circles(img)\n",
        "\n",
        "    # Paint out the first circle detected. Assume that only one circle was\n",
        "    # detected for whole image.\n",
        "    x, y, r = circles[0]\n",
        "    cv.circle(img_thresh, (x, y), r, (0, 0, 0), cv.FILLED)\n",
        "\n",
        "    # Erode what's left to try and remove edges.\n",
        "    img_thresh = cv.erode(img_thresh, np.ones((3, 3), np.uint8))\n",
        "    img_thresh = cv.dilate(img_thresh, np.ones((3, 3), np.uint8))\n",
        "\n",
        "    # Extract a region of interest that is very likely to contain the notch if\n",
        "    # one is present in the image. This corresponds to a small square at about\n",
        "    # the 45 degree mark on the eyeball. Some notches are slightly lower than\n",
        "    # this, so the ROI should be large enough to capture many notch positions.\n",
        "    ratio = 1.0 / 4.0\n",
        "    roi_size = img_thresh.shape[0] * ratio\n",
        "    half_rs = roi_size / 2.0\n",
        "\n",
        "    # Do a little trig to find cartesian coordinates of 45 degrees point.\n",
        "    radius = img_thresh.shape[0] / 2.0\n",
        "    angle = math.pi / 4.0\n",
        "    side = radius * math.sin(angle)\n",
        "\n",
        "    # Get the damned ROI.\n",
        "    x, y = (int(radius + side - half_rs), int(radius - side - half_rs))\n",
        "    cv.rectangle(img, (x, y), (x + int(roi_size), y + int(roi_size)), (255, 255, 255))\n",
        "    roi = img_thresh[y:int(y + roi_size), x:int(x + roi_size)]\n",
        "\n",
        "    # Run blob detection on what's left.\n",
        "    # Set up the detector with default parameters.\n",
        "    blob_params = cv.SimpleBlobDetector_Params()\n",
        "    blob_params.minThreshold = 0.0\n",
        "    blob_params.maxThreshold = THRESH\n",
        "    blob_params.thresholdStep = THRESH / 2\n",
        "    blob_params.filterByArea = False\n",
        "    blob_params.filterByColor = False\n",
        "    blob_params.filterByConvexity = False\n",
        "    blob_params.filterByInertia = True\n",
        "    blob_params.minInertiaRatio = 0.05\n",
        "    blob_params.maxInertiaRatio = 1\n",
        "    sbd = cv.SimpleBlobDetector_create(blob_params)\n",
        "\n",
        "    # Detect blobs.\n",
        "    keypoints = sbd.detect(roi)\n",
        "\n",
        "    # Draw circles around any detected blobs.\n",
        "    # cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS ensures the size of the circle\n",
        "    # corresponds to the size of blob\n",
        "    roi = cv.drawKeypoints(roi, keypoints, np.array([]),\n",
        "                           (0, 0, 255),\n",
        "                           cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "\n",
        "    # Image windows for this experiment.\n",
        "    o_window = \"Original Image\"\n",
        "    t_window = \"Thresholded, Subtracted, Blob-Detected\"\n",
        "    cv.namedWindow(o_window, cv.WINDOW_AUTOSIZE)\n",
        "    cv.namedWindow(t_window, cv.WINDOW_AUTOSIZE)\n",
        "    cv.imshow(o_window, img)\n",
        "    cv.imshow(t_window, roi)\n",
        "    cv.waitKey(0)\n",
        "    return\n",
        "\n",
        "\n",
        "def experiment_bounding_box(path):\n",
        "    \"\"\" Bounding box resizing experiment. \"\"\"\n",
        "\n",
        "    # Calculate initial bounding box via thresholded image.\n",
        "    img = load_image(path, resize=False)\n",
        "    img_thresh = threshold(img)\n",
        "    x, y, w, h = cv.boundingRect(img_thresh)\n",
        "\n",
        "    max_wh = max(w, h)\n",
        "    diff_w = max_wh - w\n",
        "    diff_h = max_wh - h\n",
        "    half_dw = int(math.floor(diff_w / 2.0))\n",
        "    half_dh = int(math.floor(diff_h / 2.0))\n",
        "    img_expand = np.zeros((max_wh, max_wh), dtype=np.uint8)\n",
        "    roi = img[y:y + h, x:x + w]\n",
        "    img_expand[half_dh:(half_dh + h), half_dw:(half_dw + w)] = roi\n",
        "\n",
        "    img_resize = cv.resize(img_expand,\n",
        "                           (STD_RES, STD_RES),\n",
        "                           interpolation=cv.INTER_AREA)\n",
        "\n",
        "    r_window = \"Resized Image\"\n",
        "    #cv.namedWindow(r_window, cv.WINDOW_AUTOSIZE)\n",
        "    #cv.imshow(r_window, img_resize)\n",
        "    #cv.waitKey(0)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kqK0gjEtCnAd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "c7e667ef-1c68-4ebb-ca20-7ac0a539f9a8"
      },
      "cell_type": "code",
      "source": [
        "!mkdir output\n",
        "!mkdir output/train\n",
        "!mkdir output/val"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘output’: File exists\n",
            "mkdir: cannot create directory ‘output/train’: File exists\n",
            "mkdir: cannot create directory ‘output/val’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qoipQ8g3C6cs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "\n",
        "dir_path = \"./train\"\n",
        "output_path = \"./output/train\"\n",
        "\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "\n",
        "for file_name in listing_test:\n",
        "    if \"jpeg\" in file_name:\n",
        "      if str(file_name).split(\"_\")[1]==\"right.jpeg\":\n",
        "        path = \"{}/{}\".format(dir_path, file_name)\n",
        "        output = \"{}/{}\".format(output_path, file_name)\n",
        "        new_img = preprocess_image(path)\n",
        "        cv.imwrite(output, new_img)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AqbidL5arIVu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "\n",
        "dir_path = \"./train\"\n",
        "output_path = \"./output/val\"\n",
        "\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "\n",
        "for file_name in listing_val:\n",
        "    if \"jpeg\" in file_name:\n",
        "      if str(file_name).split(\"_\")[1]==\"right.jpeg\":\n",
        "        path = \"{}/{}\".format(dir_path, file_name)\n",
        "        output = \"{}/{}\".format(output_path, file_name)\n",
        "        new_img = preprocess_image(path)\n",
        "        cv.imwrite(output, new_img)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R_Z2vUwihn1_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c1a61328-d991-4a52-ea1e-5905238ecdd3"
      },
      "cell_type": "code",
      "source": [
        "!ls output/val -1 | wc -l\n",
        "!ls output/train -1 | wc -l"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2559\n",
            "7450\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6qAqWjJlEiI4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "71412cb7-26b2-42da-a1c4-4fed97e985cc"
      },
      "cell_type": "code",
      "source": [
        "listing = os.listdir(\"output\") \n",
        "print(listing )"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['train', 'val']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dPEDFpZ_h7mQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install imblearn\n",
        "#np.set_printoptions(threshold=np.inf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "umEKCSmT1Qy8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "from keras.optimizers import SGD,RMSprop,adam\n",
        "import keras\n",
        "\n",
        "adam = keras.optimizers.Adam(lr=0.003, beta_1=0.9, beta_2=0.9999, epsilon=None, decay=0.0001, amsgrad=False)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution2D(nb_filters, nb_conv, nb_conv,\n",
        "                        border_mode='valid',\n",
        "                        input_shape=(img_cols, img_rows, 1)))\n",
        "convout1 = Activation('relu')\n",
        "model.add(convout1)\n",
        "model.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n",
        "model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
        "model.add(Dropout(0.5))\n",
        "convout2 = Activation('relu')\n",
        "model.add(convout2)\n",
        "model.add(Convolution2D(nb_filters, nb_conv-1, nb_conv-1))\n",
        "model.add(MaxPooling2D(pool_size=(nb_pool+1, nb_pool+1)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "convout3 = Activation('relu')\n",
        "model.add(convout3)\n",
        "model.add(Convolution2D(nb_filters, nb_conv-1, nb_conv-1))\n",
        "model.add(MaxPooling2D(pool_size=(nb_pool+2, nb_pool+2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(680))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(5))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy',metrics=['accuracy'], optimizer='adam')\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V0HcgSst1cpD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "9e4fdd9c-10fb-4a3c-d335-0237386b65ba"
      },
      "cell_type": "code",
      "source": [
        "!free -m"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              total        used        free      shared  buff/cache   available\r\n",
            "Mem:          13029         381        1263         247       11383       12151\r\n",
            "Swap:             0           0           0\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VhYSqVN7_k8q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "7d712405-90ac-477c-866c-57e10010518e"
      },
      "cell_type": "code",
      "source": [
        "!df -BG "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filesystem     1G-blocks  Used Available Use% Mounted on\r\n",
            "overlay             359G  125G      216G  37% /\r\n",
            "tmpfs                 7G    0G        7G   0% /dev\r\n",
            "tmpfs                 7G    0G        7G   0% /sys/fs/cgroup\r\n",
            "/dev/root             2G    1G        1G  44% /opt/bin\r\n",
            "tmpfs                 7G    1G        7G   4% /usr/lib64-nvidia\r\n",
            "/dev/sda1           365G  210G      156G  58% /etc/hosts\r\n",
            "shm                   1G    0G        1G   0% /dev/shm\r\n",
            "tmpfs                 7G    0G        7G   0% /sys/firmware\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E5_GQoOV-ePN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#second way"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SfRGzKZekssx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir output/train/0\n",
        "!mkdir output/train/1\n",
        "!mkdir output/train/2\n",
        "!mkdir output/train/3\n",
        "!mkdir output/train/4\n",
        "!mkdir output/val/0\n",
        "!mkdir output/val/1\n",
        "!mkdir output/val/2\n",
        "!mkdir output/val/3\n",
        "!mkdir output/val/4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pXrRSElFk8-H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "listing_train = os.listdir(\"output/train\")\n",
        "listing_val = os.listdir(\"output/val\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ISa-er3zwEri",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ba8a4577-99fa-4afa-bb93-761fd13b4226"
      },
      "cell_type": "code",
      "source": [
        "file=listing_train[0]\n",
        "base = os.path.basename(\"output/\" + file)\n",
        "print( os.path.splitext(file)[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1666_right\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oJ3K-PbPw3QG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "base_dir = \"output/val/\"\n",
        "\n",
        "for file in listing_val:\n",
        "  try:\n",
        "    fileName = os.path.splitext(file)[0]\n",
        "    folder_name = trainLabels.loc[trainLabels.image==fileName, 'level'].values[0]  \n",
        "    os.rename(base_dir + file, base_dir + str(folder_name) + \"/\" + file )\n",
        "  except:\n",
        "    pass\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CZsXxqA1kPei",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "base_dir = \"output/train/\"\n",
        "\n",
        "for file in listing_train:\n",
        "  try:\n",
        "    fileName = os.path.splitext(file)[0]\n",
        "    folder_name = trainLabels.loc[trainLabels.image==fileName, 'level'].values[0]  \n",
        "    os.rename(base_dir + file, base_dir + str(folder_name) + \"/\" + file )\n",
        "  except:\n",
        "    pass\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9O3YYXUO_8bd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "83f2deee-fe5f-4006-e01e-8896a8890abc"
      },
      "cell_type": "code",
      "source": [
        "from keras.applications import VGG16\n",
        " \n",
        "main_model = VGG16(weights='imagenet',\n",
        "                  include_top=False,\n",
        "                  input_shape=(224, 224, 3))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "iQ53mCYe999q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4bTfP4RW_lBV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "a308fc7a-9736-44ff-b2ed-ccb7b8d3378a"
      },
      "cell_type": "code",
      "source": [
        "train_dir =\"./output/train\"\n",
        "val_dir = \"./output/val\"\n",
        "\n",
        "datagen = ImageDataGenerator(samplewise_std_normalization=True)\n",
        "process_target = True\n",
        "\n",
        "flow_val=datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical')\n",
        "\n",
        "\n",
        "flow_train=datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical')\n",
        "\n",
        "counter =0\n",
        "\n",
        "def batch_generator_val(datagen,flow_val):\n",
        "  number_of_batches =12\n",
        "  global counter\n",
        "  while True:\n",
        "    counter = counter +1\n",
        "    x_batch = next(flow_val,counter)\n",
        "    x = main_model.predict(x_batch[0])\n",
        "    y = x_batch[1]\n",
        "    res_batch = (x,y)\n",
        "    yield res_batch\n",
        "  \n",
        "def batch_generator_train(datagen,flow_train):\n",
        "  number_of_batches =8  \n",
        "  global counter \n",
        "  while True:\n",
        "    x_batch = next(flow_train,counter)\n",
        "    x = main_model.predict(x_batch[0])\n",
        "    y = x_batch[1]\n",
        "    res_batch = (x,y)\n",
        "    yield res_batch"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d554c84684f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mval_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./output/val\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdatagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamplewise_std_normalization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprocess_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ImageDataGenerator' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Jnny20lBA3ll",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "val_generator = batch_generator_val(datagen,flow_val)\n",
        "train_generator = batch_generator_train(datagen,flow_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HuNZbbZ6ATZo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "from keras.optimizers import SGD,RMSprop,adam\n",
        "import keras\n",
        "from keras import layers, models, optimizers\n",
        "\n",
        "adam = keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.9999, epsilon=1e-07, decay=0.00001, amsgrad=False)\n",
        "\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(10000, activation='relu', input_shape=(7,7,512)))\n",
        "model.add(Flatten())\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(5, activation='softmax'))\n",
        "model.add(Dense(4000))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(600))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=2e-3),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ddvYYHJQFXDF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-nLCFM6EAlXw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "hist_plot = model.fit_generator(train_generator, steps_per_epoch=90, workers=0 ,epochs=50, validation_data=val_generator, validation_steps=30, use_multiprocessing=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fUNXjJq2ycuP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# number of convolutional filters to use\n",
        "nb_filters = 30\n",
        "# size of pooling area for max pooling\n",
        "nb_pool = 2\n",
        "# convolution kernel size\n",
        "nb_conv = 4\n",
        "\n",
        "\n",
        "# number of output classes\n",
        "nb_classes = 5\n",
        "# number of epochs to train\n",
        "nb_epoch = 64\n",
        "\n",
        "img_rows, img_cols = 750,750"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sW4Tm-N6pFHP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "from keras.optimizers import SGD,RMSprop,adam\n",
        "import keras\n",
        "\n",
        "adam = keras.optimizers.Adam(lr=3e-03, beta_1=0.9, beta_2=0.9999, epsilon=None, decay=0.00001, amsgrad=False)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution2D(nb_filters, nb_conv, nb_conv,\n",
        "                        border_mode='valid',\n",
        "                        input_shape=(img_cols, img_rows, 3)))\n",
        "convout1 = Activation('relu')\n",
        "model.add(convout1)\n",
        "model.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n",
        "model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
        "model.add(Dropout(0.5))\n",
        "convout2 = Activation('relu')\n",
        "model.add(convout2)\n",
        "model.add(Convolution2D(nb_filters, nb_conv-1, nb_conv-1))\n",
        "model.add(MaxPooling2D(pool_size=(nb_pool+1, nb_pool+1)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "convout3 = Activation('relu')\n",
        "model.add(convout3)\n",
        "model.add(Convolution2D(nb_filters, nb_conv-1, nb_conv-1))\n",
        "model.add(MaxPooling2D(pool_size=(nb_pool+2, nb_pool+2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "convout4 = Activation('relu')\n",
        "model.add(convout4)\n",
        "model.add(Convolution2D(nb_filters, nb_conv-1, nb_conv-1))\n",
        "model.add(MaxPooling2D(pool_size=(nb_pool+2, nb_pool+2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(680))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(5))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy',metrics=['accuracy'], optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "13XLt49SKSmJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(256, activation='relu', input_dim=7 * 7 * 512))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=2e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vCeXLoKFxlwG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "23_gz3wF_DI4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3S7OHKu8y218",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#hist = model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_split=0.2)\n",
        "history = model.fit_generator(train_generator, steps_per_epoch=64, epochs=1000, validation_data=val_generator, validation_steps=50)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y6Y49IsjzCb-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  !ls  train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lcoXqRB1NY5K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5DcQ8I1S0aV2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2fjg2MqL0jG9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}